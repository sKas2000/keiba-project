#!/usr/bin/env python3
"""
netkeiba.com ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ v0.1
=================================
JRA scraperã§å–å¾—ã—ãŸinput.jsonã«ã€éå»èµ°ãƒ‡ãƒ¼ã‚¿ãƒ»é¨æ‰‹æˆç¸¾ã‚’è¿½åŠ ã™ã‚‹

å¿…è¦ãªãƒ‡ãƒ¼ã‚¿:
- éå»èµ°ã®ç€é †ã€ç€å·®ã€ä¸ŠãŒã‚Š3Fã€ã‚¿ã‚¤ãƒ 
- é¨æ‰‹ã®å¹´é–“å‹ç‡ã€è¤‡å‹ç‡
- åŒã‚³ãƒ¼ã‚¹ãƒ»åŒè·é›¢ã®æˆç¸¾ï¼ˆå„ªå…ˆåº¦ä¸­ã€å°†æ¥å®Ÿè£…ï¼‰

ä½¿ã„æ–¹:
  python netkeiba_scraper.py <input.jsonã®ãƒ‘ã‚¹>
"""

VERSION = "0.1"

import asyncio
import json
import re
import sys
from pathlib import Path
from datetime import datetime

try:
    from playwright.async_api import async_playwright
except ImportError:
    print("Playwright ãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ã™:")
    print("  pip install playwright")
    print("  playwright install chromium")
    sys.exit(1)


# ============================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================================================

def safe_int(text: str) -> int:
    """æ–‡å­—åˆ—ã‚’æ•´æ•°ã«å¤‰æ›ã€‚å¤±æ•—æ™‚ã¯0"""
    try:
        return int(re.sub(r'[^\d]', '', text))
    except (ValueError, AttributeError):
        return 0


def safe_float(text: str) -> float:
    """æ–‡å­—åˆ—ã‚’æµ®å‹•å°æ•°ç‚¹ã«å¤‰æ›ã€‚å¤±æ•—æ™‚ã¯0.0"""
    try:
        return float(text.strip().replace(",", ""))
    except (ValueError, AttributeError):
        return 0.0


# ============================================================
# netkeibaã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼æœ¬ä½“
# ============================================================

class NetkeibaScraper:
    def __init__(self, headless=False, debug=True):
        self.headless = headless
        self.debug = debug
        self.pw = None
        self.browser = None
        self.ctx = None
        self.page = None

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆåŒã˜é¨æ‰‹ã‚’è¤‡æ•°å›æ¤œç´¢ã—ãªã„ãŸã‚ï¼‰
        self.jockey_cache = {}

    def log(self, msg):
        if self.debug:
            print(f"  [DEBUG] {msg}")

    async def start(self):
        """ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•"""
        self.pw = await async_playwright().start()
        self.browser = await self.pw.chromium.launch(headless=self.headless)
        self.ctx = await self.browser.new_context(
            viewport={"width": 1280, "height": 900},
            locale="ja-JP",
        )
        self.page = await self.ctx.new_page()
        self.page.set_default_timeout(15000)

    async def close(self):
        """ãƒ–ãƒ©ã‚¦ã‚¶çµ‚äº†"""
        if self.ctx:
            await self.ctx.close()
        if self.browser:
            await self.browser.close()
        if self.pw:
            await self.pw.stop()

    # ----------------------------------------------------------
    # é¦¬åæ¤œç´¢ â†’ é¦¬è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—
    # ----------------------------------------------------------

    async def search_horse(self, horse_name: str) -> str:
        """
        netkeibaã§é¦¬åæ¤œç´¢ã—ã€é¦¬è©³ç´°ãƒšãƒ¼ã‚¸ã®URLã‚’è¿”ã™
        Returns: é¦¬è©³ç´°ãƒšãƒ¼ã‚¸URL or ""
        """
        self.log(f"é¦¬åæ¤œç´¢: {horse_name}")

        try:
            # netkeibaæ¤œç´¢ãƒšãƒ¼ã‚¸
            search_url = f"https://db.netkeiba.com/?pid=horse_search_detail&word={horse_name}"
            await self.page.goto(search_url, wait_until="domcontentloaded")
            await asyncio.sleep(1)

            # æ¤œç´¢çµæœã‹ã‚‰æœ€åˆã®é¦¬ãƒªãƒ³ã‚¯ã‚’å–å¾—
            # netkeibaã®é¦¬ãƒªãƒ³ã‚¯ã¯ /horse/ ã‚’å«ã‚€
            links = await self.page.locator("a[href*='/horse/']").all()

            if not links:
                self.log(f"  âš ï¸ é¦¬ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {horse_name}")
                return ""

            # æœ€åˆã®é¦¬è©³ç´°ãƒªãƒ³ã‚¯ã‚’å–å¾—
            first_link = links[0]
            href = await first_link.get_attribute("href")

            if href:
                # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                if href.startswith("/"):
                    href = f"https://db.netkeiba.com{href}"
                self.log(f"  âœ… é¦¬ãƒšãƒ¼ã‚¸: {href}")
                return href

        except Exception as e:
            self.log(f"  âŒ é¦¬æ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({horse_name}): {e}")

        return ""

    # ----------------------------------------------------------
    # éå»èµ°ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ¼ã‚¹
    # ----------------------------------------------------------

    async def scrape_horse_past_races(self, horse_url: str) -> list:
        """
        é¦¬è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰éå»èµ°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        Returns: [{"date": "", "venue": "", "race": "", "distance": "", "surface": "",
                   "finish": 1, "margin": "0.2", "time": "1:23.4", "last3f": "34.5"}, ...]
        """
        self.log(f"éå»èµ°ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­: {horse_url}")

        past_races = []

        try:
            await self.page.goto(horse_url, wait_until="domcontentloaded")
            await asyncio.sleep(1.5)

            # netkeibaã®ç«¶èµ°æˆç¸¾ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
            # db_h_race_results ã‚¯ãƒ©ã‚¹ã‚’æŒã¤ãƒ†ãƒ¼ãƒ–ãƒ«ã€ã¾ãŸã¯"ç€é †"ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«
            tables = await self.page.locator("table").all()

            result_table = None
            for table in tables:
                # ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
                ths = await table.locator("th").all()
                headers = []
                for th in ths:
                    text = await th.text_content()
                    headers.append(text.strip() if text else "")

                # "ç€é †" ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ãŒç«¶èµ°æˆç¸¾
                if any("ç€é †" in h for h in headers):
                    result_table = table
                    self.log(f"  â˜… ç«¶èµ°æˆç¸¾ãƒ†ãƒ¼ãƒ–ãƒ«ç™ºè¦‹ (åˆ—æ•°: {len(headers)})")

                    # ãƒ˜ãƒƒãƒ€ãƒ¼ä½ç½®ã‚’è¨˜éŒ²
                    header_map = {}
                    for i, h in enumerate(headers):
                        if "æ—¥ä»˜" in h:
                            header_map["date"] = i
                        elif "é–‹å‚¬" in h or "ç«¶é¦¬å ´" in h:
                            header_map["venue"] = i
                        elif "ãƒ¬ãƒ¼ã‚¹å" in h:
                            header_map["race"] = i
                        elif "è·é›¢" in h:
                            header_map["distance"] = i
                        elif "é¦¬å ´" in h:
                            header_map["surface"] = i
                        elif "ç€é †" in h:
                            header_map["finish"] = i
                        elif "ã‚¿ã‚¤ãƒ " in h:
                            header_map["time"] = i
                        elif "ç€å·®" in h:
                            header_map["margin"] = i
                        elif "ä¸ŠãŒã‚Š" in h or "ä¸Šã‚Š" in h:
                            header_map["last3f"] = i
                        elif "é€šé" in h:
                            header_map["position"] = i

                    self.log(f"  ãƒ˜ãƒƒãƒ€ãƒ¼ãƒãƒƒãƒ—: {header_map}")
                    break

            if not result_table:
                self.log(f"  âš ï¸ ç«¶èµ°æˆç¸¾ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return past_races

            # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’å–å¾—ï¼ˆæœ€æ–°4èµ°ï¼‰
            rows = await result_table.locator("tbody tr").all()
            if not rows:
                rows = await result_table.locator("tr").all()
                # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                rows = [r for r in rows if await r.locator("th").count() == 0]

            for row_idx, row in enumerate(rows[:4]):
                cells = await row.locator("td").all()

                if len(cells) < 5:  # æœ€ä½é™ã®ãƒ‡ãƒ¼ã‚¿ãŒãªã„
                    continue

                # ã‚»ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                cell_texts = []
                for cell in cells:
                    text = await cell.text_content()
                    cell_texts.append(text.strip() if text else "")

                race_data = {
                    "date": "",
                    "venue": "",
                    "race": "",
                    "distance": "",
                    "surface": "",
                    "finish": 0,
                    "margin": "",
                    "time": "",
                    "last3f": "",
                    "position": "",
                }

                # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒãƒƒãƒ—ã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
                if "date" in header_map and header_map["date"] < len(cell_texts):
                    race_data["date"] = cell_texts[header_map["date"]]

                if "venue" in header_map and header_map["venue"] < len(cell_texts):
                    venue_text = cell_texts[header_map["venue"]]
                    # "3å›æ±äº¬1æ—¥" â†’ "æ±äº¬" ã‚’æŠ½å‡º
                    venue_match = re.search(r'\d+å›(.+?)\d+æ—¥', venue_text)
                    race_data["venue"] = venue_match.group(1) if venue_match else venue_text

                if "race" in header_map and header_map["race"] < len(cell_texts):
                    race_data["race"] = cell_texts[header_map["race"]]

                if "distance" in header_map and header_map["distance"] < len(cell_texts):
                    dist_text = cell_texts[header_map["distance"]]
                    # "ãƒ€1200" or "èŠ1600" â†’ é¦¬å ´ã¨distanceã‚’åˆ†é›¢
                    dist_match = re.match(r'([èŠãƒ€éšœ])(\d+)', dist_text)
                    if dist_match:
                        race_data["surface"] = dist_match.group(1)
                        race_data["distance"] = dist_match.group(2)
                    else:
                        race_data["distance"] = re.sub(r'[^\d]', '', dist_text)

                if "surface" in header_map and header_map["surface"] < len(cell_texts):
                    if not race_data["surface"]:  # distanceã‹ã‚‰å–å¾—ã—ã¦ã„ãªã„å ´åˆ
                        race_data["surface"] = cell_texts[header_map["surface"]]

                if "finish" in header_map and header_map["finish"] < len(cell_texts):
                    finish_text = cell_texts[header_map["finish"]]
                    race_data["finish"] = safe_int(finish_text)

                if "time" in header_map and header_map["time"] < len(cell_texts):
                    race_data["time"] = cell_texts[header_map["time"]]

                if "margin" in header_map and header_map["margin"] < len(cell_texts):
                    race_data["margin"] = cell_texts[header_map["margin"]]

                if "last3f" in header_map and header_map["last3f"] < len(cell_texts):
                    race_data["last3f"] = cell_texts[header_map["last3f"]]

                if "position" in header_map and header_map["position"] < len(cell_texts):
                    race_data["position"] = cell_texts[header_map["position"]]

                # ç€é †ãŒå–å¾—ã§ããŸå ´åˆã®ã¿è¿½åŠ 
                if race_data["finish"] > 0 or race_data["date"]:
                    past_races.append(race_data)
                    self.log(f"    [{row_idx+1}] {race_data['date']} {race_data['venue']} {race_data['finish']}ç€")

            self.log(f"  â†’ éå»èµ° {len(past_races)}ä»¶å–å¾—")

        except Exception as e:
            self.log(f"  âŒ éå»èµ°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()

        return past_races

    # ----------------------------------------------------------
    # é¨æ‰‹æ¤œç´¢ â†’ å¹´é–“æˆç¸¾å–å¾—
    # ----------------------------------------------------------

    async def search_jockey(self, jockey_name: str) -> dict:
        """
        é¨æ‰‹åã§æ¤œç´¢ã—ã€å¹´é–“æˆç¸¾ã‚’å–å¾—
        Returns: {"win_rate": 0.15, "place_rate": 0.35, "wins": 50, "races": 300}
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if jockey_name in self.jockey_cache:
            self.log(f"é¨æ‰‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {jockey_name}")
            return self.jockey_cache[jockey_name]

        self.log(f"é¨æ‰‹æ¤œç´¢: {jockey_name}")

        result = {"win_rate": 0.0, "place_rate": 0.0, "wins": 0, "races": 0}

        try:
            # netkeibaé¨æ‰‹æ¤œç´¢
            search_url = f"https://db.netkeiba.com/?pid=jockey_search_detail&word={jockey_name}"
            await self.page.goto(search_url, wait_until="domcontentloaded")
            await asyncio.sleep(1)

            # é¨æ‰‹è©³ç´°ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—
            links = await self.page.locator("a[href*='/jockey/']").all()

            if not links:
                self.log(f"  âš ï¸ é¨æ‰‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {jockey_name}")
                self.jockey_cache[jockey_name] = result
                return result

            # æœ€åˆã®é¨æ‰‹ãƒªãƒ³ã‚¯ã‚’å–å¾—
            first_link = links[0]
            href = await first_link.get_attribute("href")

            if href:
                if href.startswith("/"):
                    href = f"https://db.netkeiba.com{href}"

                self.log(f"  â†’ é¨æ‰‹ãƒšãƒ¼ã‚¸: {href}")

                # é¨æ‰‹ãƒšãƒ¼ã‚¸ã‹ã‚‰æˆç¸¾ã‚’å–å¾—
                await self.page.goto(href, wait_until="domcontentloaded")
                await asyncio.sleep(1.5)

                # å¹´é–“æˆç¸¾ã‚’æ¢ã™
                # æ–¹æ³•1: ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å‹ç‡ãƒ»è¤‡å‹ç‡ã‚’ç›´æ¥å–å¾—
                tables = await self.page.locator("table").all()

                for table in tables:
                    text = await table.text_content()

                    # æœ¬å¹´æˆç¸¾ã‚„é€šç®—æˆç¸¾ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
                    if "æœ¬å¹´" in text or "2026å¹´" in text or "æˆç¸¾" in text:
                        # å‹ç‡ã®æŠ½å‡ºï¼ˆä¾‹: "å‹ç‡ 0.150" or "15.0%"ï¼‰
                        win_match = re.search(r'å‹ç‡[^\d]*([\d.]+)', text)
                        if win_match:
                            win_val = safe_float(win_match.group(1))
                            # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨è¨˜ï¼ˆ15.0ï¼‰ã‹å°æ•°è¡¨è¨˜ï¼ˆ0.15ï¼‰ã‹åˆ¤å®š
                            result["win_rate"] = win_val / 100 if win_val > 1 else win_val

                        # è¤‡å‹ç‡ã®æŠ½å‡º
                        place_match = re.search(r'è¤‡å‹ç‡[^\d]*([\d.]+)', text)
                        if place_match:
                            place_val = safe_float(place_match.group(1))
                            result["place_rate"] = place_val / 100 if place_val > 1 else place_val

                        # å‹åˆ©æ•°ã¨å‡ºèµ°æ•°
                        wins_match = re.search(r'(\d+)\s*å‹', text)
                        if wins_match:
                            result["wins"] = safe_int(wins_match.group(1))

                        races_match = re.search(r'(\d+)\s*æˆ¦', text)
                        if races_match:
                            result["races"] = safe_int(races_match.group(1))

                        if result["win_rate"] > 0:
                            break

                # æ–¹æ³•2: å‹ç‡ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã€ãƒšãƒ¼ã‚¸å…¨ä½“ã‹ã‚‰æ¢ã™
                if result["win_rate"] == 0:
                    page_text = await self.page.text_content("body")

                    # å…¨ä½“ã‹ã‚‰å‹ç‡ã‚’æ¤œç´¢
                    win_match = re.search(r'å‹ç‡[^\d]*([\d.]+)', page_text)
                    if win_match:
                        win_val = safe_float(win_match.group(1))
                        result["win_rate"] = win_val / 100 if win_val > 1 else win_val

                    place_match = re.search(r'è¤‡å‹ç‡[^\d]*([\d.]+)', page_text)
                    if place_match:
                        place_val = safe_float(place_match.group(1))
                        result["place_rate"] = place_val / 100 if place_val > 1 else place_val

                if result["win_rate"] > 0 or result["place_rate"] > 0:
                    self.log(f"  âœ… å‹ç‡{result['win_rate']:.3f} è¤‡å‹ç‡{result['place_rate']:.3f}")
                else:
                    self.log(f"  âš ï¸ æˆç¸¾ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        except Exception as e:
            self.log(f"  âŒ é¨æ‰‹æ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({jockey_name}): {e}")
            import traceback
            traceback.print_exc()

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self.jockey_cache[jockey_name] = result
        return result


# ============================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================================================

async def enrich_race_data(input_path: str):
    """
    input.jsonã‚’èª­ã¿è¾¼ã¿ã€netkeibaã‹ã‚‰è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦enriched_input.jsonã«ä¿å­˜
    """
    print()
    print("=" * 60)
    print(f"  ğŸ´ netkeiba ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ v{VERSION}")
    print("=" * 60)
    print()

    # input.jsonèª­ã¿è¾¼ã¿
    input_file = Path(input_path)
    if not input_file.exists():
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_path}")
        return

    print(f"ğŸ“‚ å…¥åŠ›: {input_file}")

    with open(input_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    horses = data.get("horses", [])
    print(f"ğŸ´ å¯¾è±¡é¦¬: {len(horses)}é ­")

    scraper = NetkeibaScraper(headless=False, debug=True)

    try:
        await scraper.start()

        # å„é¦¬ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ‹¡å¼µ
        for i, horse in enumerate(horses):
            horse_name = horse.get("name", "")
            print(f"\n[{i+1}/{len(horses)}] {horse_name}")

            if not horse_name:
                print("  âš ï¸ é¦¬åãŒç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                horse["past_races"] = []
                horse["jockey_stats"] = {"win_rate": 0.0, "place_rate": 0.0, "wins": 0, "races": 0}
                continue

            # é¦¬è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—
            horse_url = await scraper.search_horse(horse_name)

            if horse_url:
                # éå»èµ°ãƒ‡ãƒ¼ã‚¿å–å¾—
                past_races = await scraper.scrape_horse_past_races(horse_url)
                horse["past_races"] = past_races
            else:
                horse["past_races"] = []
                print(f"  âš ï¸ éå»èµ°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

            # é¨æ‰‹æˆç¸¾å–å¾—
            jockey_name = horse.get("jockey", "")
            if jockey_name:
                print(f"  é¨æ‰‹: {jockey_name}")
                jockey_stats = await scraper.search_jockey(jockey_name)
                horse["jockey_stats"] = jockey_stats
            else:
                print(f"  âš ï¸ é¨æ‰‹æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆinput.jsonã«'jockey'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå¿…è¦ï¼‰")
                horse["jockey_stats"] = {"win_rate": 0.0, "place_rate": 0.0, "wins": 0, "races": 0}

            await asyncio.sleep(0.5)  # è² è·è»½æ¸›

        # ä¿å­˜
        output_file = input_file.parent / input_file.name.replace("_input.json", "_enriched_input.json")

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"\n{'=' * 60}")
        print(f"  âœ… ä¿å­˜å®Œäº†: {output_file}")
        print(f"{'=' * 60}")

    except KeyboardInterrupt:
        print("\nâš ï¸ ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await scraper.close()


async def main():
    if len(sys.argv) < 2:
        print("ä½¿ã„æ–¹: python netkeiba_scraper.py <input.jsonã®ãƒ‘ã‚¹>")
        print("ä¾‹: python netkeiba_scraper.py ../data/races/20260214_æ±äº¬_4R_input.json")
        sys.exit(1)

    input_path = sys.argv[1]
    await enrich_race_data(input_path)


if __name__ == "__main__":
    asyncio.run(main())
